{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\" > Classification of Startup Viability </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Introduction**\n",
    "\n",
    "The objective of this project is to develop a classification model that identifies entities registered with StartupBlink as either \"Viable\" or \"Not Viable.\" In this context, \"Viable\" refers to those entities that meet the data cleaning criteria established by StartupBlink and are thus retained in the system, while \"Not Viable\" signifies those that do not meet these standards and are consequently removed.\n",
    "\n",
    "To achieve this classification, I utilized two datasets: test_task.csv and cleaned_test_task.csv.\n",
    "\n",
    "test_task.csv is the original dataset provided by StartupBlink for the assessment of my skills in data cleaning and  processing.\n",
    "cleaned_test_task.csv is the resultant dataset generated after applying the specified data cleaning metrics outlined by StartupBlink to the original dataset.\n",
    "\n",
    "The classification of entities into \"Viable\" and \"Not Viable\" is determined by a binary target variable called classified. This column was created by extracting the UUIDs of the entities present in the cleaned dataset, which are classified as 1 (indicating viability), while those UUIDs not found in the cleaned dataset are classified as 0 (indicating non-viability) in the original dataset.\n",
    "\n",
    "The independent variables used to construct the classification models include description, categories_list, city, region, country, continent, and tld. This comprehensive approach ensures that the model leverages key contextual information to make accurate viability predictions.\n",
    "\n",
    "Five different models were developed to identify the most effective approach for classification:\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from string import punctuation, digits\n",
    "from nltk.corpus import stopwords\n",
    "import ast\n",
    "\n",
    "stopword = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv files into DataFrames\n",
    "df = pd.read_csv(\"test_task.csv\")\n",
    "df_cleaned = pd.read_csv(\"cleaned_test_task.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>uuid</th>\n",
       "      <th>website</th>\n",
       "      <th>Name</th>\n",
       "      <th>description</th>\n",
       "      <th>valuation_usd</th>\n",
       "      <th>categories</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5639c94c-408b-450a-9694-df238f6d9da9</td>\n",
       "      <td>https://www.perplexity.ai</td>\n",
       "      <td>Perplexity AI</td>\n",
       "      <td>Perplexity is a search engine platform that us...</td>\n",
       "      <td>5.200000e+08</td>\n",
       "      <td>[{'entity_def_id': 'category', 'permalink': 'a...</td>\n",
       "      <td>[{'permalink': 'san-francisco-california', 'uu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00e32424-3cc3-4798-8361-78914c212fb0</td>\n",
       "      <td>https://mistral.ai</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>Mistral AI is a platform that assembles team t...</td>\n",
       "      <td>1.999462e+09</td>\n",
       "      <td>[{'entity_def_id': 'category', 'permalink': 'a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c8326e69-b364-44af-a7a7-1f9239d532f1</td>\n",
       "      <td>https://robinai.co.uk</td>\n",
       "      <td>Robin AI</td>\n",
       "      <td>Robin AI is a legal infrastructure business th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'entity_def_id': 'category', 'permalink': 'a...</td>\n",
       "      <td>[{'permalink': 'london-england', 'uuid': 'aad1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cf2c678c-b81a-80c3-10d1-9c5e76448e51</td>\n",
       "      <td>https://www.openai.com</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>OpenAI is an AI research and deployment compan...</td>\n",
       "      <td>2.900000e+10</td>\n",
       "      <td>[{'entity_def_id': 'category', 'permalink': 'a...</td>\n",
       "      <td>[{'permalink': 'san-francisco-california', 'uu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0f3fd8ce-9fe3-d6bd-d243-ffdf2fcfb012</td>\n",
       "      <td>http://www.startengine.com</td>\n",
       "      <td>StartEngine</td>\n",
       "      <td>StartEngine is an equity crowdfunding platform...</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>[{'entity_def_id': 'category', 'permalink': 'c...</td>\n",
       "      <td>[{'permalink': 'los-angeles-california', 'uuid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  uuid  \\\n",
       "0           0  5639c94c-408b-450a-9694-df238f6d9da9   \n",
       "1           1  00e32424-3cc3-4798-8361-78914c212fb0   \n",
       "2           2  c8326e69-b364-44af-a7a7-1f9239d532f1   \n",
       "3           3  cf2c678c-b81a-80c3-10d1-9c5e76448e51   \n",
       "4           4  0f3fd8ce-9fe3-d6bd-d243-ffdf2fcfb012   \n",
       "\n",
       "                      website           Name  \\\n",
       "0   https://www.perplexity.ai  Perplexity AI   \n",
       "1          https://mistral.ai     Mistral AI   \n",
       "2       https://robinai.co.uk       Robin AI   \n",
       "3      https://www.openai.com         OpenAI   \n",
       "4  http://www.startengine.com    StartEngine   \n",
       "\n",
       "                                         description  valuation_usd  \\\n",
       "0  Perplexity is a search engine platform that us...   5.200000e+08   \n",
       "1  Mistral AI is a platform that assembles team t...   1.999462e+09   \n",
       "2  Robin AI is a legal infrastructure business th...            NaN   \n",
       "3  OpenAI is an AI research and deployment compan...   2.900000e+10   \n",
       "4  StartEngine is an equity crowdfunding platform...   1.200000e+08   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [{'entity_def_id': 'category', 'permalink': 'a...   \n",
       "1  [{'entity_def_id': 'category', 'permalink': 'a...   \n",
       "2  [{'entity_def_id': 'category', 'permalink': 'a...   \n",
       "3  [{'entity_def_id': 'category', 'permalink': 'a...   \n",
       "4  [{'entity_def_id': 'category', 'permalink': 'c...   \n",
       "\n",
       "                                            location  \n",
       "0  [{'permalink': 'san-francisco-california', 'uu...  \n",
       "1                                                NaN  \n",
       "2  [{'permalink': 'london-england', 'uuid': 'aad1...  \n",
       "3  [{'permalink': 'san-francisco-california', 'uu...  \n",
       "4  [{'permalink': 'los-angeles-california', 'uuid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning and Feature Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the unnamed column as it holds no significance\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# removing the valuation column because there are too many missing values\n",
    "df = df.drop(columns=[\"valuation_usd\"])\n",
    "\n",
    "# removing rows with missing values in the following columns below\n",
    "df = df.dropna(subset=['website', 'description', 'categories', 'location'])\n",
    "\n",
    "# converting the string representation of lists to actual lists of dictionaries\n",
    "df['categories'] = df['categories'].apply(ast.literal_eval)\n",
    "# creating a new column (categories_list) by extracting and joining the \"value\" key into a single string\n",
    "df['categories_list'] = df['categories'].apply(\n",
    "    lambda x: ', '.join([category['value'] for category in x]) if isinstance(x, list) else ''\n",
    ")\n",
    "# Ensure the new column is of object dtype (string)\n",
    "df['categories_list'] = df['categories_list'].astype('object')\n",
    "# droping categories column as it is no longer needed\n",
    "df = df.drop(columns=['categories'])\n",
    "\n",
    "\n",
    "# defining a function to extract the value based on location_type\n",
    "def extract_location(locations, loc_type):\n",
    "    # Extract the first location value if available, else return None\n",
    "    values = [loc['value'] for loc in locations if loc['location_type'] == loc_type]\n",
    "    return values[0] if values else None\n",
    "\n",
    "# converting location column into actual lists of dictionaries\n",
    "df['location'] = df['location'].apply(ast.literal_eval)\n",
    "# creating separate columns for city, region, and country\n",
    "df['city'] = df['location'].apply(lambda x: extract_location(x, 'city'))\n",
    "df['region'] = df['location'].apply(lambda x: extract_location(x, 'region'))\n",
    "df['country'] = df['location'].apply(lambda x: extract_location(x, 'country'))\n",
    "df['continent'] = df['location'].apply(lambda x: extract_location(x, 'continent'))\n",
    "# droping location column as it is not needed\n",
    "df = df.drop(columns=['location'])\n",
    "\n",
    "# creating a new column (tld) by extrating the top level domains from the website column\n",
    "df['tld'] = df['website'].str.extract(r'(?:https?://)?(?:www\\.)?[^/]+\\.(\\w+)', expand=False) # using regex\n",
    "# droping website column\n",
    "df = df.drop(columns=['website'])\n",
    "\n",
    "# creating a set of unique values from df_cleaned\n",
    "df_cleaned_set = set(df_cleaned['uuid'])\n",
    "\n",
    "# creating the 'classified' column in df based on the presence in df_cleaned\n",
    "df['classified'] = df['uuid'].apply(lambda x: 1 if x in df_cleaned_set else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>Name</th>\n",
       "      <th>description</th>\n",
       "      <th>categories_list</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>tld</th>\n",
       "      <th>classified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5639c94c-408b-450a-9694-df238f6d9da9</td>\n",
       "      <td>Perplexity AI</td>\n",
       "      <td>Perplexity is a search engine platform that us...</td>\n",
       "      <td>Artificial Intelligence (AI), Chatbot, Generat...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>ai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c8326e69-b364-44af-a7a7-1f9239d532f1</td>\n",
       "      <td>Robin AI</td>\n",
       "      <td>Robin AI is a legal infrastructure business th...</td>\n",
       "      <td>Artificial Intelligence (AI), Contact Manageme...</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>uk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cf2c678c-b81a-80c3-10d1-9c5e76448e51</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>OpenAI is an AI research and deployment compan...</td>\n",
       "      <td>Artificial Intelligence (AI), Generative AI, M...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0f3fd8ce-9fe3-d6bd-d243-ffdf2fcfb012</td>\n",
       "      <td>StartEngine</td>\n",
       "      <td>StartEngine is an equity crowdfunding platform...</td>\n",
       "      <td>Crowdfunding, Finance, Financial Services, Fin...</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>99cc593e-96ee-48fe-824a-0ebc12a94a63</td>\n",
       "      <td>bitsCrunch</td>\n",
       "      <td>AI-enhanced decentralized data networks delive...</td>\n",
       "      <td>Analytics, Artificial Intelligence (AI), Block...</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Europe</td>\n",
       "      <td>com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid           Name  \\\n",
       "0  5639c94c-408b-450a-9694-df238f6d9da9  Perplexity AI   \n",
       "2  c8326e69-b364-44af-a7a7-1f9239d532f1       Robin AI   \n",
       "3  cf2c678c-b81a-80c3-10d1-9c5e76448e51         OpenAI   \n",
       "4  0f3fd8ce-9fe3-d6bd-d243-ffdf2fcfb012    StartEngine   \n",
       "5  99cc593e-96ee-48fe-824a-0ebc12a94a63     bitsCrunch   \n",
       "\n",
       "                                         description  \\\n",
       "0  Perplexity is a search engine platform that us...   \n",
       "2  Robin AI is a legal infrastructure business th...   \n",
       "3  OpenAI is an AI research and deployment compan...   \n",
       "4  StartEngine is an equity crowdfunding platform...   \n",
       "5  AI-enhanced decentralized data networks delive...   \n",
       "\n",
       "                                     categories_list           city  \\\n",
       "0  Artificial Intelligence (AI), Chatbot, Generat...  San Francisco   \n",
       "2  Artificial Intelligence (AI), Contact Manageme...         London   \n",
       "3  Artificial Intelligence (AI), Generative AI, M...  San Francisco   \n",
       "4  Crowdfunding, Finance, Financial Services, Fin...    Los Angeles   \n",
       "5  Analytics, Artificial Intelligence (AI), Block...         Munich   \n",
       "\n",
       "       region         country      continent  tld  classified  \n",
       "0  California   United States  North America   ai           1  \n",
       "2     England  United Kingdom         Europe   uk           0  \n",
       "3  California   United States  North America  com           0  \n",
       "4  California   United States  North America  com           0  \n",
       "5      Bayern         Germany         Europe  com           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classified\n",
       "0    6476\n",
       "1    3321\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"classified\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation of Independent variables for model building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining relevant text columns into a single \"document\" per row\n",
    "text_columns = ['description', 'categories_list', 'city', 'region', 'country', 'continent', 'tld']\n",
    "df['combined_text'] = df[text_columns].fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Perplexity is a search engine platform that uses artificial intelligence to provide large language models and search engines. The company's platform enables the development of beneficial artificial general intelligence, as well as an open-source environment that is accessible to the public, allowing clients to develop skills and knowledge in artificial intelligence. Artificial Intelligence (AI), Chatbot, Generative AI, Search Engine, Software San Francisco California United States North America ai\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"combined_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Building a Bag-of-Words (BoW) model***\n",
    "\n",
    "Three functions have been defined to process the text data for machine learning. \n",
    "\n",
    "1. extract_words(text)\n",
    "\n",
    "This function tokenizes input text by isolating punctuation and digits, converting the text to lowercase, and splitting it into individual words. The processed words can then be used in further text analysis. It is a helper function for the bag_of_words function.\n",
    "\n",
    "2. bag_of_words(texts, remove_stopword=False)\n",
    "\n",
    "This function builds a vocabulary dictionary from a list of text documents. Each unique word gets assigned a unique index, which allows for representing each word in a feature vector (used for bag-of-words). If remove_stopword is True, it excludes words found in a stopword list.\n",
    "\n",
    "3. extract_bow_feature_vectors(reviews, indices_by_word, binarize=True)\n",
    "\n",
    "This function generates a feature matrix representing the Bag-of-Words model for a list of text documents. Each row corresponds to a document, and each column represents a word from the vocabulary dictionary. The function also allows for binarization, which indicates word presence or absence (useful for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by isolating punctuation and digits, converting all text to lowercase,\n",
    "    and splitting the text into individual words.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input string to process.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lowercase words with punctuation and digits separated.\n",
    "    \"\"\"\n",
    "    for c in punctuation + digits:\n",
    "        text = text.replace(c, ' ' + c + ' ')\n",
    "    return text.lower().split()\n",
    "\n",
    "def bag_of_words(texts, remove_stopword=False):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary dictionary from a list of texts. Each unique word is assigned an index,\n",
    "    allowing it to be represented in feature vectors. Optionally excludes stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list of str): A list of text documents to process.\n",
    "    remove_stopword (bool): If True, excludes words found in the 'stopword' list.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping each unique word to a unique index, forming the vocabulary.\n",
    "    \"\"\"\n",
    "    indices_by_word = {}\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if remove_stopword and word in stopword:  # Remove if in stopwords\n",
    "                continue\n",
    "            if word not in indices_by_word:\n",
    "                indices_by_word[word] = len(indices_by_word)\n",
    "    return indices_by_word\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, indices_by_word, binarize=True):\n",
    "    \"\"\"\n",
    "    Creates a feature matrix for a list of reviews based on the provided vocabulary.\n",
    "    Each row represents a document, and each column corresponds to a word in the vocabulary.\n",
    "    Optionally binarizes the matrix to indicate word presence/absence.\n",
    "\n",
    "    Parameters:\n",
    "    reviews (list of str): A list of text documents to process.\n",
    "    indices_by_word (dict): A dictionary mapping words to unique indices (vocabulary).\n",
    "    binarize (bool): If True, converts non-zero entries in the feature matrix to 1.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A feature matrix with rows representing documents and columns representing words.\n",
    "    \"\"\"\n",
    "    feature_matrix = np.zeros([len(reviews), len(indices_by_word)], dtype=np.float64)\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word not in indices_by_word: \n",
    "                continue\n",
    "            feature_matrix[i, indices_by_word[word]] += 1\n",
    "    if binarize:\n",
    "        feature_matrix[feature_matrix > 0] = 1\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Dependent (y) and Independent (X_bow) Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating bag-of-words indices and feature matrix\n",
    "combined_texts = df['combined_text'].tolist()\n",
    "indices_by_word = bag_of_words(combined_texts)\n",
    "X_bow = extract_bow_feature_vectors(combined_texts, indices_by_word)\n",
    "\n",
    "# preparing target labels for logistic regression\n",
    "y = df['classified'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Development and Evaluation (Logistic Regression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6770\n",
      "Precision: 0.5396\n",
      "Recall: 0.4897\n",
      "F1 Score: 0.5135\n",
      "Confusion Matrix:\n",
      "[[993 285]\n",
      " [348 334]]\n"
     ]
    }
   ],
   "source": [
    "# spliting the dataframe into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculating performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# displaying the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Devlopment and Evaluation (Decision Trees)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6276\n",
      "Precision: 0.4613\n",
      "Recall: 0.4194\n",
      "F1 Score: 0.4393\n",
      "Confusion Matrix:\n",
      "[[944 334]\n",
      " [396 286]]\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Development and Evaluation (Random Forest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6852\n",
      "Precision: 0.5895\n",
      "Recall: 0.3138\n",
      "F1 Score: 0.4096\n",
      "Confusion Matrix:\n",
      "[[1129  149]\n",
      " [ 468  214]]\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6526\n",
      "Precision: 0.5007\n",
      "Recall: 0.5191\n",
      "F1 Score: 0.5097\n",
      "Confusion Matrix:\n",
      "[[925 353]\n",
      " [328 354]]\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM classifier with a linear kernel\n",
    "model = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Development and Evaluation(Neural Network)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 10:54:37.531006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/holykan/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 74ms/step - accuracy: 0.6659 - loss: 0.6521 - val_accuracy: 0.6594 - val_loss: 0.6117\n",
      "Epoch 2/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - accuracy: 0.8170 - loss: 0.4063 - val_accuracy: 0.6735 - val_loss: 0.6337\n",
      "Epoch 3/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 87ms/step - accuracy: 0.9494 - loss: 0.1450 - val_accuracy: 0.6824 - val_loss: 0.8908\n",
      "Epoch 4/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - accuracy: 0.9940 - loss: 0.0293 - val_accuracy: 0.6811 - val_loss: 1.1955\n",
      "Epoch 5/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - accuracy: 0.9998 - loss: 0.0052 - val_accuracy: 0.6913 - val_loss: 1.4065\n",
      "Epoch 6/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6862 - val_loss: 1.5053\n",
      "Epoch 7/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 7.5338e-04 - val_accuracy: 0.6888 - val_loss: 1.5667\n",
      "Epoch 8/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.5128e-04 - val_accuracy: 0.6888 - val_loss: 1.6259\n",
      "Epoch 9/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 3.0489e-04 - val_accuracy: 0.6862 - val_loss: 1.6802\n",
      "Epoch 10/10\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 2.1575e-04 - val_accuracy: 0.6862 - val_loss: 1.7140\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "Accuracy: 0.6668\n",
      "Precision: 0.5251\n",
      "Recall: 0.4443\n",
      "F1 Score: 0.4813\n",
      "Confusion Matrix:\n",
      "[[1004  274]\n",
      " [ 379  303]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, class_weight=class_weight_dict)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Best Model (Random Forest)**\n",
    "\n",
    "The Random Forest model using class weights produced the best performance metrics:\n",
    "\n",
    "**Accuracy**: 0.6852\n",
    "This indicates that approximately 68.52% of the predictions made by the model were correct. While this accuracy might seem relatively good, it can still be improved upon. \n",
    "\n",
    "**Precision**: 0.5895\n",
    "This precision score means that when the model predicted an entity as \"Viable\" (class 1), it was correct about 58.95% of the time. This indicates a moderate level of confidence in the positive predictions.\n",
    "\n",
    "**Recall**: 0.3138\n",
    "The recall score of 31.38% signifies that the model correctly identified only about 31.38% of all actual \"Viable\" entities (class 1). This indicates that the model may be missing a significant portion of the viable entities, which is a critical concern.\n",
    "\n",
    "**F1 Score**: 0.4096\n",
    "The F1 Score, which balances precision and recall, is 0.4096. This value reflects the trade-off between precision and recall, highlighting that while the model has reasonable precision, its ability to recall viable instances is quite low.\n",
    "\n",
    "**Confusion Matrix**:\n",
    "- **True Negatives (TN)**: 1129 – The model correctly identified 1129 entities as \"Not Viable\" (class 0).\n",
    "- **False Positives (FP)**: 149 – The model incorrectly predicted 149 entities as \"Viable\" (class 1) when they were actually \"Not Viable.\"\n",
    "- **False Negatives (FN)**: 468 – The model failed to identify 468 actual \"Viable\" entities, predicting them as \"Not Viable.\"\n",
    "- **True Positives (TP)**: 214 – The model correctly identified 214 entities as \"Viable.\"\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Overall, while the Random Forest model with weighted classes achieved decent accuracy and precision, its low recall indicates a significant issue with identifying viable entities. Efforts to improve recall, such as further tuning the model or exploring different algorithms, may be necessary to enhance its predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
